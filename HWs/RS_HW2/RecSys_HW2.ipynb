{"cells":[{"cell_type":"markdown","source":["### Installing\n"],"metadata":{"id":"iBQzirX7EIPh"}},{"cell_type":"code","source":["# polara and gitfiles\n","!pip -q install --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara\n","! wget -q https://raw.githubusercontent.com/Personalization-Technologies-Lab/RecSys-Course-HSE-Fall23/main/Seminar5/dataprep.py -O dataprep.py\n","! wget -q https://raw.githubusercontent.com/Personalization-Technologies-Lab/RecSys-Course-HSE-Fall23/main/Seminar5/evaluation.py -O evaluation.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Dmi7RGrDajH","executionInfo":{"status":"ok","timestamp":1702568813490,"user_tz":-180,"elapsed":18519,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"e6acf9d1-5091-48d4-f697-3a662dd1e8be"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for polara (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"j2lI3V4SE9Zb"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"GfYLoqf-DSi2","executionInfo":{"status":"ok","timestamp":1702568816557,"user_tz":-180,"elapsed":3073,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tqdm\n","from scipy.sparse import csr_matrix, diags\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from polara import get_movielens_data\n","\n","from dataprep import leave_last_out, transform_indices, reindex_data, verify_time_split, generate_interactions_matrix\n","from evaluation import topn_recommendations, model_evaluate, downvote_seen_items"]},{"cell_type":"markdown","source":["# Problem 1 (10 pts)"],"metadata":{"id":"lqW8pW9GNgx5"}},{"cell_type":"markdown","source":["You’re given the matrix of interactions between 3 users and 6 items:\n","```\n","1 0 0 1 0 0\n","0 0 1 0 0 1\n","0 1 0 0 1 0\n","```\n","Is it possible to build a personalized recommendation model with this data?\n","Explain your answer.\n"],"metadata":{"id":"T_CootJBNmvS"}},{"cell_type":"markdown","source":["I think no, to build personalazed model we dont have enough data.\n","To build a personalized recommendation model, it's essential to have more information than just this interaction matrix. Typically, recommendation models require additional data, such as user demographics, item features, or explicit user feedback (e.g., ratings).\n","\n","In this case, without additional information, it would be challenging to build a traditional personalized recommendation model. We have only intersection matrix, without users or item features, so we can use ordinary approaches such as User-Based Collaborative Filtering, SVD, Matrix Factorizatio, K-Nearest Neighbors (KNN) Based Recommender Systems or others.\n","\n","But in this case the matrix doesn't have needed information to create good rec. model cause nor users preferensys, nor item sequenses dosent intersect and dont have some logical connections that matrix based model can use to fited.\n","In summary, the absence of user-item intersections poses a fundamental challenge for recommendation models These models are designed to capture patterns and relationships within the interaction matrix, and when no interactions are observed, they lack the necessary information to generate personalized recommendations."],"metadata":{"id":"uVica8W2OL6Q"}},{"cell_type":"markdown","metadata":{"id":"BqZXc9P7DSi5"},"source":["# Problem 2 (20 pts)\n"]},{"cell_type":"markdown","metadata":{"id":"cjaSdH4cDSi7"},"source":["Implement two variants of user-based KNN for the top-$n$ recommendations task when similarity matrix is calculated:\n","1. with neighborhood subsampling,\n","2. with additional weighting.\n","\n","Recall, there's no reason for implementing row-wise weighting scheme in user-based KNN. So choose the weighting scheme wisely.\n","\n"," In your experiments:   \n","\n","- Use Movielens-1M data.\n","- Test your solution against both weak and strong generalization.\n","  - In total you’ll have 4 different experiments.\n","- Follow the ”most-recent-item” sampling strategy for constructing holdout.\n","  - Explain potential issues of this scheme in relation to both weak and strong\n","generalization.\n","- Report evaluation metrics, compare the models, and analyse the results\n","\n","**Note**: you can reuse the code from seminars if necessary."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ejvkvfR9DSi8","executionInfo":{"status":"ok","timestamp":1702568818284,"user_tz":-180,"elapsed":1736,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"13c7a108-b3d3-4dac-9639-c09dc032bd1a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'users': 'userid',\n"," 'items': 'movieid',\n"," 'feedback': 'rating',\n"," 'n_users': 6040,\n"," 'n_items': 3706}"]},"metadata":{},"execution_count":3}],"source":["data_ = get_movielens_data(include_time=True)\n","data, data_index = transform_indices(data_, 'userid', 'movieid')\n","\n","\n","data_description = dict(\n","    users = 'userid',\n","    items = 'movieid',\n","    feedback = 'rating',\n","    n_users = len(data.userid.unique()),\n","    n_items = len(data.movieid.unique())\n",")\n","data_description"]},{"cell_type":"markdown","metadata":{"id":"r6hzfEvjDSi8"},"source":["## Weak generalization test"]},{"cell_type":"markdown","metadata":{"id":"YH3bWQlMDSi9"},"source":["### Preparing data (1 pts)"]},{"cell_type":"markdown","metadata":{"id":"V-2mQNXqDSi9"},"source":["Your task is\n","- split data into training and holdout parts\n","- build a new internal contiguous representation of user and item index based on the training data\n","- make sure same index is used in the holdout data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpcaIQjhDSi-"},"outputs":[],"source":["# split most recent holdout item from each user\n","training_, holdout_ = leave_last_out(data, 'userid', 'timestamp')\n","\n","# check correct time splitting\n","verify_time_split(training_, holdout_)"]},{"cell_type":"code","source":["assert holdout_.userid.unique().shape == training_.userid.unique().shape"],"metadata":{"id":"ofr6TA5nai4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVkzlDfqDSi_"},"outputs":[],"source":["# reindex data to make contiguous index starting from 0 for user and item IDs\n","training, data_index = transform_indices(training_, 'userid', 'movieid')\n","\n","# apply new index to the holdout data\n","holdout = reindex_data(holdout_, data_index, filter_invalid=True)\n","holdout = holdout.sort_values('userid')\n","#assert set(holdout.userid) == set(training.userid)"]},{"cell_type":"markdown","metadata":{"id":"vGIytWTfDSi_"},"source":["- Let's also populate data description dictionary for convenience.\n","- It allows using uniform names for users and items field.\n","  - This way the code does't depend on the actual names in you dataset.\n","  - So later you can easily switch to another dataset without changing the code fo the pipeline.\n"]},{"cell_type":"code","source":["#data_description['test_users'] = holdout[data_index['users'].name].values\n","#data_description\n","data_description = dict(\n","    users = data_index['users'].name,\n","    items = data_index['items'].name,\n","    feedback = 'rating',\n","    n_users = len(data_index['users']),\n","    n_items = len(data_index['items']),\n","    test_users = holdout[data_index['users'].name].values\n",")\n","data_description"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-EAY4eYSQZRB","executionInfo":{"status":"ok","timestamp":1701715144286,"user_tz":-180,"elapsed":14,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"c7332307-917d-49f7-88c9-78881a8214f4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'users': 'userid',\n"," 'items': 'movieid',\n"," 'feedback': 'rating',\n"," 'n_users': 6040,\n"," 'n_items': 3704,\n"," 'test_users': array([   0,    1,    2, ..., 6037, 6038, 6039])}"]},"metadata":{},"execution_count":98}]},{"cell_type":"markdown","metadata":{"id":"xUc-WsL6DSjA"},"source":["As previously, let's also explicitly store our testset (i.e., ratings of test users excluding holdout items)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPQwGIu1DSjB"},"outputs":[],"source":["userid = data_description['users']\n","seen_idx_mask = training[userid].isin(data_description['test_users'])\n","testset = training[seen_idx_mask]"]},{"cell_type":"markdown","metadata":{"id":"-jU7AfyoDSjB"},"source":["### Models implementation"]},{"cell_type":"markdown","metadata":{"id":"H7ANzQFmDSjB"},"source":["#### Unweighted case (5 pts) and Weighted case (5 pts)"]},{"cell_type":"markdown","metadata":{"id":"vylkAW3HDSjC"},"source":["Unweighted case\n","- You can consult the code from seminars or implement your own solution as long as it is fast enough.  \n","- **Make sure to implement some kind of neighborhood subsampling.**\n","  - Recall that subsampling of the neighborhood not only makes the algorithm run faster, but can also improve the results.\n","\n","Weighted case\n","- Your task here is to implement user-based KNN with asymmetric similarity."]},{"cell_type":"code","source":["def generate_interactions_matrix(data, data_description, rebase_users=False):\n","    \"csr matrix user-item intersection, value of cell is rating by user\"\n","    '''\n","    Convert pandas dataframe with interactions into a sparse matrix.\n","    Allows reindexing user ids, which help ensure data consistency\n","    at the scoring stage (assumes user ids are sorted in scoring array).\n","    '''\n","    n_users = data_description['n_users']\n","    n_items = data_description['n_items']\n","    # get indices of observed data\n","    user_idx = data[data_description['users']].values # type your code here\n","    if rebase_users:\n","        user_idx, user_index = pd.factorize(user_idx, sort=True)\n","        n_users = len(user_index)\n","    item_idx = data[data_description['items']].values # type your code here\n","    feedback = data[data_description['feedback']].values # type your code here\n","    # construct rating matrix\n","    return csr_matrix((feedback, (user_idx, item_idx)), shape=(n_users, n_items))\n","\n","def cosine_similarity_zd(matrix):\n","    '''Build cosine similarity matrix with zero diagonal.'''\n","    similarity = cosine_similarity(matrix, dense_output=False) # type your code here\n","    similarity.setdiag(0)\n","    similarity.eliminate_zeros()\n","    return similarity\n","\n","def truncate_similarity(similarity, k):\n","    '''\n","    For every row in similarity matrix, pick at most k entities\n","    with the highest similarity scores. Disregard everything else.\n","    '''\n","    similarity = similarity.tocsr()\n","    inds = similarity.indices\n","    ptrs = similarity.indptr\n","    data = similarity.data\n","    new_ptrs = [0]\n","    new_inds = []\n","    new_data = []\n","    for i in range(len(ptrs)-1):\n","        start, stop = ptrs[i], ptrs[i+1]\n","        if start < stop:\n","            data_ = data[start:stop]\n","            topk = min(len(data_), k)\n","            idx = np.argpartition(data_, -topk)[-topk:]\n","            new_data.append(data_[idx])\n","            new_inds.append(inds[idx+start])\n","            new_ptrs.append(new_ptrs[-1]+len(idx))\n","        else:\n","            new_ptrs.append(new_ptrs[-1])\n","    new_data = np.concatenate(new_data)\n","    new_inds = np.concatenate(new_inds)\n","    truncated = csr_matrix(\n","        (new_data, new_inds, new_ptrs),\n","        shape=similarity.shape\n","    )\n","    return truncated"],"metadata":{"id":"UAspP_WXvV6x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If $A$ is a matrix of ratings and  $K$ is an user-similarity matrix ($k_{ij}\\in[0, 1]$), then KNN-scores matrix $R$ is computed as:  \n","\n","- for elementwise weighting:\n","$$\n","R=K A  \\oslash\\left(K B \\right),\\quad\n","b_{u i}=\\left\\{\\begin{array}{lr}\n","1, & \\text { if } a_{u i} \\text { is known } \\\\\n","0 & \\text { otherwise }\n","\\end{array}\\right.\n","$$\n","\n","- for row-wise weighting:\n","$$\n","R=D_K^{-1} K A ,\\quad\n","D_K=\\operatorname{diag}(K\\mathbf{e})\n","$$\n","\n","- for unweighted case:\n","$$\n","R=K A\n","$$"],"metadata":{"id":"UXCE42zBy7cV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hz63kP5fDSjC"},"outputs":[],"source":["def build_uknn_model(config, data, data_description):\n","    user_item_mtx = generate_interactions_matrix(data, data_description, rebase_users=False)\n","\n","    # compute similarity matrix\n","    user_similarity = cosine_similarity_zd(user_item_mtx)\n","    # truncate similarity from irrelefant users\n","    if config['n_neighbors']:\n","        user_similarity = truncate_similarity( user_similarity, config['n_neighbors'])\n","    return user_item_mtx, user_similarity, config['weighting']\n","\n","\n","def uknn_model_scoring(params, testset, testset_description):\n","    \"\"\"\n","    params = [user_item_mtx_interections, user_similarity_matrix, weighting_type]\n","    \"\"\"\n","    # implement the scoring function to assign scores\n","    # to all items for test users\n","    user_item_mtx, user_similarity, weighting = params\n","\n","    # write your code for scoring, don't forget to return a dense array\n","    user_item_mtx = generate_interactions_matrix(\n","        testset, testset_description, rebase_users=False\n","    )\n","\n","    # R_unw = K*A\n","    unweighted_scores = user_similarity.dot(user_item_mtx)\n","\n","    if weighting is None:\n","        # R_unw = K*A\n","        return unweighted_scores.A\n","\n","    if weighting == 'el_wise' and False:\n","        # R_elw = K*A // K*B\n","        # R_elw = R_unw // weights\n","        weights = np.abs(user_similarity).dot(user_item_mtx.astype('bool')) # почему мы здесь берем модуль?\n","        return np.divide(\n","            unweighted_scores.A,\n","            weights.A,\n","            where=weights.A!=0\n","        )\n","\n","    if weighting == 'row_wise' and False:\n","        # R_rw = Dk^-1*K*A\n","        # R_rw = R_unw*weights\n","        weights = np.abs(user_similarity).sum(axis=1).A.squeeze()\n","        Dk1 = np.divide(1., weights, where=weights!=0)\n","        scores = diags(Dk1)@ user_similarity  @ user_item_mtx\n","        return  scores.A\n","\n","    if weighting == 'col_wise':\n","        # R_rw = Dk^-1*K*A\n","        # R_rw = R_unw*weights\n","        weights = np.abs(user_similarity).sum(axis=1).A.squeeze()\n","        Dk1 = np.divide(1., weights, where=weights!=0)\n","        scores = user_similarity @ diags(Dk1) @ user_item_mtx\n","        return  scores.A\n","\n","\n","    raise ValueError('Unrecognized weighting scheme')"]},{"cell_type":"code","source":["%%time\n","import timeit\n","\n","n_neighbors = 10\n","models_names_list = ['unweihgted', 'colwise_weihgted']\n","models_names_list += [i+f'_trunc{n_neighbors}' for i in models_names_list]\n","\n","conf_list = [{'weighting': None, 'n_neighbors': None},\n","             {'weighting': 'col_wise', 'n_neighbors': None},\n","             {'weighting': None, 'n_neighbors': n_neighbors},\n","             {'weighting': 'col_wise', 'n_neighbors': n_neighbors}\n","             ]\n","models_param_list = []\n","models_scores_list = []\n","\n","for name, conf in zip(models_names_list, conf_list):\n","    print(f'##########_____{name}_____##########')\n","    start = timeit.default_timer()\n","\n","    uknn_params = build_uknn_model(conf, training, data_description)\n","    uknn_scores = uknn_model_scoring(uknn_params, testset, data_description)\n","    models_param_list.append(uknn_params)\n","    models_scores_list.append(uknn_scores)\n","\n","    stop = timeit.default_timer()\n","    print(f'Time: {stop - start:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5QF_zYG6Ukc","executionInfo":{"status":"ok","timestamp":1701712301867,"user_tz":-180,"elapsed":100714,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"33256315-f19f-496b-a3b9-7f90d6b8a576"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##########_____unweihgted_____##########\n","Time: 36.26\n","##########_____colwise_weihgted_____##########\n","Time: 51.89\n","##########_____unweihgted_trunc10_____##########\n","Time: 5.81\n","##########_____colwise_weihgted_trunc10_____##########\n","Time: 7.08\n","CPU times: user 1min 26s, sys: 1.88 s, total: 1min 28s\n","Wall time: 1min 41s\n"]}]},{"cell_type":"markdown","metadata":{"id":"_p9cuOS0DSjE"},"source":["### Evaluation (1 pts)"]},{"cell_type":"markdown","metadata":{"id":"tLbFgDioDSjE"},"source":["Generate top-$n$ recommendations for both models and Calculate metrics"]},{"cell_type":"markdown","source":["Function wich delete scored items of users"],"metadata":{"id":"kEeI2iqBUIbu"}},{"cell_type":"code","source":["def downvote_seen_items(scores, data, data_description):\n","    assert isinstance(scores, np.ndarray), 'Scores must be a dense numpy array!'\n","    itemid = data_description['items']\n","    userid = data_description['users']\n","    # get indices of observed data, corresponding to scores array\n","    # we need to provide correct mapping of rows in scores array into\n","    # the corresponding user index (which is assumed to be sorted)\n","    #row_idx, test_users = pd.factorize(data[userid], sort=True)\n","    #print(len(test_users) , scores.shape[0])\n","    #assert len(test_users) == scores.shape[0]\n","    row_idx = data[userid].values\n","    col_idx = data[itemid].values\n","    # downvote scores at the corresponding positions\n","    scores[row_idx, col_idx] = scores.min() - 1"],"metadata":{"id":"MCPHq_LHVYg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fuction to calculate metrics from our predictions and holdout"],"metadata":{"id":"_0xTDQ9ZJCqQ"}},{"cell_type":"code","source":["def model_evaluate(recommended_items, holdout, holdout_description, topn=10):\n","    itemid = holdout_description['items']\n","    holdout_items = holdout[itemid].values\n","    assert recommended_items.shape[0] == len(holdout_items)\n","    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n","    # HR calculation\n","    hr = np.mean(hits_mask.any(axis=1))\n","    # MRR calculation\n","    n_test_users = recommended_items.shape[0]\n","    hit_rank = np.where(hits_mask)[1] + 1.0\n","    mrr = np.sum(1 / hit_rank) / n_test_users\n","    # coverage calculation\n","    n_items = holdout_description['n_items']\n","    cov = np.unique(recommended_items).size / n_items\n","    return hr, mrr, cov"],"metadata":{"id":"i3q_3_nxoVjU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get metrics from our models"],"metadata":{"id":"PZhFlNWkIw9Y"}},{"cell_type":"code","source":["metrics_list = []\n","\n","for name, conf, scores in zip(models_names_list, conf_list, models_scores_list):\n","    print(f'##########_____{name}_____##########')\n","    print(conf)\n","\n","    uknn_scores = scores.copy()\n","    # delete scored items\n","    downvote_seen_items(uknn_scores, testset, data_description)\n","    # take topN recomendastions\n","    uknn_recs = topn_recommendations(uknn_scores)\n","\n","    metrics = model_evaluate(uknn_recs[holdout.userid.tolist()], holdout, data_description, topn=10)\n","    metrics_list.append(metrics)\n","    print('HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkT5S-WpA9Xs","executionInfo":{"status":"ok","timestamp":1701712303501,"user_tz":-180,"elapsed":1647,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"701017c0-9f00-4c6d-8c2b-cbf535f34174"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##########_____unweihgted_____##########\n","{'weighting': None, 'n_neighbors': None}\n","HR=0.0474, MRR=0.0169, COV=0.0526\n","\n","##########_____colwise_weihgted_____##########\n","{'weighting': 'col_wise', 'n_neighbors': None}\n","HR=0.0487, MRR=0.0174, COV=0.0508\n","\n","##########_____unweihgted_trunc10_____##########\n","{'weighting': None, 'n_neighbors': 10}\n","HR=0.0831, MRR=0.0285, COV=0.365\n","\n","##########_____colwise_weihgted_trunc10_____##########\n","{'weighting': 'col_wise', 'n_neighbors': 10}\n","HR=0.0826, MRR=0.0283, COV=0.378\n","\n"]}]},{"cell_type":"code","source":["n_neighbors_list =  list(range(1, 15)) + [20,25,30,40,50,60]\n","\n","for n_neighbors in n_neighbors_list:\n","    print(f'##########_____N_neighbors:{n_neighbors}_____##########')\n","    start = timeit.default_timer()\n","    for weight in [None, 'col_wise']:\n","        conf = {'weighting': weight, 'n_neighbors': n_neighbors}\n","        uknn_params = build_uknn_model(conf, training, data_description)\n","        uknn_scores = uknn_model_scoring(uknn_params, testset, data_description)\n","\n","        # delete scored items\n","        downvote_seen_items(uknn_scores, testset, data_description)\n","        # take topN recomendastions\n","        uknn_recs = topn_recommendations(uknn_scores)\n","        metrics = model_evaluate(uknn_recs[holdout.userid.tolist()], holdout, data_description, topn=10)\n","        print(f'weighting:{weight}')\n","        print('HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics))\n","\n","    stop = timeit.default_timer()\n","    print(f'Time: {stop - start:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JiwbCC9OGsAe","executionInfo":{"status":"ok","timestamp":1701715386056,"user_tz":-180,"elapsed":224485,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"4f18f3d4-6653-4614-cd04-6f4dd7637202"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##########_____N_neighbors:1_____##########\n","weighting:None\n","HR=0.0489, MRR=0.014, COV=0.701\n","\n","weighting:col_wise\n","HR=0.0489, MRR=0.014, COV=0.701\n","\n","Time: 9.40\n","##########_____N_neighbors:2_____##########\n","weighting:None\n","HR=0.0628, MRR=0.0225, COV=0.591\n","\n","weighting:col_wise\n","HR=0.0611, MRR=0.0216, COV=0.599\n","\n","Time: 8.74\n","##########_____N_neighbors:3_____##########\n","weighting:None\n","HR=0.0681, MRR=0.0251, COV=0.525\n","\n","weighting:col_wise\n","HR=0.0696, MRR=0.0251, COV=0.536\n","\n","Time: 10.02\n","##########_____N_neighbors:4_____##########\n","weighting:None\n","HR=0.0757, MRR=0.025, COV=0.479\n","\n","weighting:col_wise\n","HR=0.075, MRR=0.0245, COV=0.49\n","\n","Time: 10.08\n","##########_____N_neighbors:5_____##########\n","weighting:None\n","HR=0.0798, MRR=0.0269, COV=0.446\n","\n","weighting:col_wise\n","HR=0.0787, MRR=0.0267, COV=0.454\n","\n","Time: 9.08\n","##########_____N_neighbors:6_____##########\n","weighting:None\n","HR=0.0808, MRR=0.0263, COV=0.426\n","\n","weighting:col_wise\n","HR=0.0821, MRR=0.026, COV=0.433\n","\n","Time: 12.37\n","##########_____N_neighbors:7_____##########\n","weighting:None\n","HR=0.0792, MRR=0.0285, COV=0.408\n","\n","weighting:col_wise\n","HR=0.081, MRR=0.0277, COV=0.418\n","\n","Time: 12.49\n","##########_____N_neighbors:8_____##########\n","weighting:None\n","HR=0.0793, MRR=0.0283, COV=0.387\n","\n","weighting:col_wise\n","HR=0.079, MRR=0.0278, COV=0.397\n","\n","Time: 10.99\n","##########_____N_neighbors:9_____##########\n","weighting:None\n","HR=0.0797, MRR=0.0285, COV=0.376\n","\n","weighting:col_wise\n","HR=0.0795, MRR=0.0278, COV=0.386\n","\n","Time: 9.51\n","##########_____N_neighbors:10_____##########\n","weighting:None\n","HR=0.0831, MRR=0.0285, COV=0.365\n","\n","weighting:col_wise\n","HR=0.0826, MRR=0.0283, COV=0.378\n","\n","Time: 10.24\n","##########_____N_neighbors:11_____##########\n","weighting:None\n","HR=0.0816, MRR=0.0287, COV=0.356\n","\n","weighting:col_wise\n","HR=0.0833, MRR=0.0285, COV=0.368\n","\n","Time: 10.63\n","##########_____N_neighbors:12_____##########\n","weighting:None\n","HR=0.0821, MRR=0.0285, COV=0.347\n","\n","weighting:col_wise\n","HR=0.0823, MRR=0.0291, COV=0.36\n","\n","Time: 10.21\n","##########_____N_neighbors:13_____##########\n","weighting:None\n","HR=0.0835, MRR=0.0288, COV=0.339\n","\n","weighting:col_wise\n","HR=0.0843, MRR=0.0292, COV=0.346\n","\n","Time: 9.72\n","##########_____N_neighbors:14_____##########\n","weighting:None\n","HR=0.0835, MRR=0.0291, COV=0.33\n","\n","weighting:col_wise\n","HR=0.0826, MRR=0.0287, COV=0.338\n","\n","Time: 12.46\n","##########_____N_neighbors:20_____##########\n","weighting:None\n","HR=0.0845, MRR=0.0308, COV=0.301\n","\n","weighting:col_wise\n","HR=0.0865, MRR=0.0308, COV=0.313\n","\n","Time: 14.45\n","##########_____N_neighbors:25_____##########\n","weighting:None\n","HR=0.0856, MRR=0.0304, COV=0.278\n","\n","weighting:col_wise\n","HR=0.0876, MRR=0.0313, COV=0.291\n","\n","Time: 14.91\n","##########_____N_neighbors:30_____##########\n","weighting:None\n","HR=0.0848, MRR=0.0306, COV=0.261\n","\n","weighting:col_wise\n","HR=0.0874, MRR=0.0311, COV=0.273\n","\n","Time: 13.00\n","##########_____N_neighbors:40_____##########\n","weighting:None\n","HR=0.0851, MRR=0.0303, COV=0.24\n","\n","weighting:col_wise\n","HR=0.0856, MRR=0.0305, COV=0.25\n","\n","Time: 12.63\n","##########_____N_neighbors:50_____##########\n","weighting:None\n","HR=0.0858, MRR=0.0307, COV=0.224\n","\n","weighting:col_wise\n","HR=0.0874, MRR=0.0309, COV=0.233\n","\n","Time: 11.73\n","##########_____N_neighbors:60_____##########\n","weighting:None\n","HR=0.0861, MRR=0.0308, COV=0.213\n","\n","weighting:col_wise\n","HR=0.0866, MRR=0.0312, COV=0.225\n","\n","Time: 11.48\n"]}]},{"cell_type":"markdown","source":["Upper n=6 we have very goof results. The main reason is that we use weak generalization. And after a certain threshold, the model already has enough information for such quality"],"metadata":{"id":"4Af9CG0DJb6n"}},{"cell_type":"markdown","metadata":{"id":"bSVECqToDSjG"},"source":["## Strong generalization test"]},{"cell_type":"markdown","metadata":{"id":"0JvDid2MDSjG"},"source":["- Recall that in the strong generalization test you work with the warm-start scenario.\n","- It means that the set of test users is disjoint from the set of users in the training.\n","- You're provided with the basic functions to help you perform correct splitting, but there're still a few places where your input is required. Make sure you understand the logic of data splitting in this scenario."]},{"cell_type":"markdown","metadata":{"id":"SNwlqScbDSjG"},"source":["### Preparing data (2 pts)"]},{"cell_type":"markdown","metadata":{"id":"RPwpP7DdDSjG"},"source":["- Your task is to select **a subset of users who have the most recent interactions in their history** across entire dataset. These are going to be the **test users**.\n","- You will apply **holdout splitting to only this subset**.\n","  - Think, why simply taking all users (as in weak generalization test) makes no sense in this scenario."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSKcVlzmDSjH"},"outputs":[],"source":["def split_by_time(data, time_q=0.95, timeid='timestamp'):\n","    '''\n","    Split the input `data` DataFrame into two parts based on the timestamp, with the split point\n","    being determined by the quantile value `time_q`. The function returns a tuple `(before, after)`\n","    containing the two DataFrames. The `after` DataFrame contains the rows with timestamps greater\n","    than or equal to the split point, while the `before` DataFrame contains the remaining rows.\n","\n","    Details:\n","    The `quantile` method of the pandas DataFrame is used to calculate the time point (i.e., timestamp)\n","    that divides the data into two parts based on the given quantile value `time_q`. Specifically,\n","    the time point `split_timepoint` is calculated as the `time_q`th quantile of the values in the `timeid`\n","    column of the `data` DataFrame, using the interpolation method of `nearest`. This means that\n","    `split_timepoint` is the timestamp at or immediately after which `time_q` percent of the data points occur.\n","    '''\n","    split_timepoint = data[timeid].quantile(q=time_q, interpolation='nearest')\n","    after = data.query(f'{timeid} >= @split_timepoint')\n","    before = data.drop(after.index)\n","    return before, after"]},{"cell_type":"markdown","metadata":{"id":"prRxC6AhDSjH"},"source":["Firstly, you need to select a candidate subset of observations, from which you'll construct the the training, testset, and holdout datssets. Check the `split_by_time` function below and its description in the above cell."]},{"cell_type":"code","source":["data_ = get_movielens_data(include_time=True)\n","data, data_index = transform_indices(data_, 'userid', 'movieid')\n","\n","data_description = dict(\n","    users = 'userid',\n","    items = 'movieid',\n","    feedback = 'rating',\n","    n_users = len(data.userid.unique()),\n","    n_items = len(data.movieid.unique())\n",")\n","data_description"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLSPKRi9K3s7","executionInfo":{"status":"ok","timestamp":1701712696229,"user_tz":-180,"elapsed":1623,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"f0a575ce-4344-49ed-e380-8b18b3747d6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'users': 'userid',\n"," 'items': 'movieid',\n"," 'feedback': 'rating',\n"," 'n_users': 6040,\n"," 'n_items': 3706}"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kP5tsXVDSjH"},"outputs":[],"source":["before, after = split_by_time(data, time_q=0.95)"]},{"cell_type":"markdown","metadata":{"id":"jnNftAfcDSjH"},"source":["- Now it's time to perform holdout sampling based on the obtained timepoint splitting.\n","- Remember, you only sample from the test users.\n","  - Test users's last ratings must be the most recent across the entire dataset. Use the global timepoint splitting obtained above."]},{"cell_type":"markdown","source":["Make holdout sampling on after to create holdout data and testset_part_1"],"metadata":{"id":"uhzc_Z4hLZGl"}},{"cell_type":"code","source":["testset_part_1, holdout_ = leave_last_out(after, userid='userid', timeid='timestamp')\n","\n","# verify correctness of time-based splitting,\n","# i.e., for each test user, the holdout contains only future interactions w.r.t to testset\n","verify_time_split(testset_part_1, holdout_)\n","assert len(set(testset_part_1.userid.unique()) - set(holdout_.userid.unique()))==0"],"metadata":{"id":"xmBCym3bLBYz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7PQgNY8uDSjO"},"source":["- Prepare the data for training.\n","  - Take the correspoding part of the timepoint split.\n","  - Recall that **training and testset must be disjoint by users**."]},{"cell_type":"markdown","source":["Then for strong generalisation we must delete all after(holdout) users from before data history, so we get our trainset"],"metadata":{"id":"5sNxmBYHLRR3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bd05tamQDSjO"},"outputs":[],"source":["trainset_ = before[~before.userid.isin(after.userid.unique())]"]},{"cell_type":"markdown","source":["And all deleted rows is our testset_part_2"],"metadata":{"id":"nkcUah-sLPmQ"}},{"cell_type":"code","source":["testset_part_2 = before[before.userid.isin(after.userid.unique())]"],"metadata":{"id":"Tw3CN4JwLO5G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0b4GdZyDSjO"},"source":["- Note that `testset_part_` only contains interactions of the test users **after the timepoint**.\n","- You need to combine it with the remaining histories of these users.\n","  - i.e., everything that's filtered out from the training data"]},{"cell_type":"code","source":["# combine all test users data into a single `testset_` Dataframe.\n","testset_ = pd.concat([testset_part_1, testset_part_2], axis=0, ignore_index=False)\n","assert len(set(testset_.userid)) == len((set(holdout_.userid)))\n","assert len(set(testset_.userid).difference((set(holdout_.userid)))) == 0\n","assert len(set(holdout_.userid).difference((set(testset_.userid)))) == 0"],"metadata":{"id":"3ZXf3_ZyLjbl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WSNusKGCDSjP"},"source":["#### Building internal representation of user and item index"]},{"cell_type":"markdown","metadata":{"id":"CQcjfhcYDSjP"},"source":["Use the `transform_indices` function for building a contiguous index starting from 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a77p078ZDSjP"},"outputs":[],"source":["# reindex data to make contiguous index starting from 0 for user and item IDs\n","#trainset, train_data_index = transform_indices(trainset_, 'userid', 'movieid') нам не нужно реиндексировать фильмы\n","\n","train_user_idx, train_user_index = pd.factorize(trainset_['userid'], sort=True)\n","\n","trainset = trainset_.copy()\n","trainset['userid'] = train_user_idx\n","assert len(set(trainset.userid)) == len(set(trainset_.userid))"]},{"cell_type":"code","source":["train_data_description = dict(\n","    users = 'userid',\n","    items = 'movieid',\n","    feedback = 'rating',\n","    n_users = len(train_user_index),\n","    n_items = data_description['n_items']\n",")"],"metadata":{"id":"zzhZ9yU1MEPs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RT_MdfCbDSjQ"},"source":["- Before applying new index to the test data note that:\n","  - the users in the `testset` must be the same as the users in the `holdout`;\n","  - the users in both `testset` and `holdout` must be ordered the same way.\n","- Below is the corresponding function `align_test_by_users` that ensures these two datasets' alignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBRFtJxoDSjQ"},"outputs":[],"source":["def align_test_by_users(testset, holdout):\n","    test_users = np.intersect1d(holdout['userid'].values, testset['userid'].values)\n","    # only allow the same users to be present in both datasets\n","    testset = testset.query('userid in @test_users').sort_values('userid')\n","    holdout = holdout.query('userid in @test_users').sort_values('userid')\n","    return testset, holdout"]},{"cell_type":"markdown","metadata":{"id":"bWAoKhNHDSjQ"},"source":["Let's apply new item index to test data and finalize the test split:"]},{"cell_type":"code","source":["test_data_description = dict(\n","    users = 'userid',\n","    items = 'movieid',\n","    feedback = 'rating',\n","    n_users = len(testset_.userid.unique()),\n","    n_items = data_description['n_items']\n",")\n","print(test_data_description)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"inIeVrHIk7aS","executionInfo":{"status":"ok","timestamp":1701712707259,"user_tz":-180,"elapsed":8,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"42e667aa-f36e-4837-db81-8d4950be8421"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'users': 'userid', 'items': 'movieid', 'feedback': 'rating', 'n_users': 813, 'n_items': 3706}\n"]}]},{"cell_type":"code","source":["test_user_idx, test_user_index = pd.factorize(testset_['userid'], sort=True)\n","\n","testset = testset_.copy()\n","testset['userid'] = test_user_idx\n","\n","holdout_reind = holdout_.copy()\n","hold_user_indx, hold_user_index = pd.factorize(holdout_.userid, sort=True)\n","assert list(hold_user_index) == list(test_user_index)\n","holdout_reind['userid'] = hold_user_indx"],"metadata":{"id":"xQFagIxCkxm-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EnluvJe0DSjR"},"source":["- Think why we do not apply new index to users here."]},{"cell_type":"markdown","metadata":{"id":"r_-3hkyuDSjR"},"source":["### Models implementation"]},{"cell_type":"markdown","metadata":{"id":"JLaExZphDSjR"},"source":["- In this section you'll need to implement user-based KNN models for the warm-start scenario.\n","- Think carefully which data must be generated at the build time and which data must be generated in the scoring function.\n","  - Recall that test users are not part of the training data.\n","- The notes on neighborhood subsampling remain the same as before."]},{"cell_type":"markdown","metadata":{"id":"toBMSLqgDSjS"},"source":["#### Unweighted case (5 pts) and Weighted case (5 pts)"]},{"cell_type":"code","source":["def cosine_similarity_warm_start(matrix_test, matrix_train):\n","    '''Build cosine similarity matrix with zero diagonal.'''\n","    similarity = cosine_similarity(matrix_test, matrix_train, dense_output=False) # type your code here\n","    similarity.setdiag(0)\n","    similarity.eliminate_zeros()\n","    return similarity"],"metadata":{"id":"Hgtah8TgljT_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f27u9qQtDSjS"},"outputs":[],"source":["def build_uknn_model(config, data, data_description):\n","    user_item_mtx = generate_interactions_matrix(data, data_description, rebase_users=False)\n","    # compute similarity matrix\n","    user_similarity = cosine_similarity_zd(user_item_mtx)\n","    # truncate similarity from irrelefant users\n","    if config['n_neighbors']:\n","        user_similarity = truncate_similarity(user_similarity, config['n_neighbors'])\n","    return user_item_mtx, user_similarity, config\n","\n","def uknn_model_scoring(params, testset, testset_description):\n","    \"\"\"\n","    params = [user_item_mtx_interections, user_similarity_matrix, weighting_type]\n","    \"\"\"\n","    # implement the scoring function to assign scores\n","    # to all items for test users\n","    user_item_mtx, user_similarity_train, config = params\n","\n","    # write your code for scoring, don't forget to return a dense array\n","    user_item_mtx_test = generate_interactions_matrix(\n","        testset, testset_description, rebase_users=True\n","    )\n","    user_similarity = cosine_similarity_warm_start(user_item_mtx_test, user_item_mtx) # test_users*train_users\n","    if config['n_neighbors']:\n","        user_similarity = truncate_similarity(user_similarity, config['n_neighbors'])\n","\n","    # R_unw = K*A\n","    unweighted_scores = user_similarity.dot(user_item_mtx)\n","\n","    if config['weighting'] is None:\n","        # R_unw = K*A\n","        return unweighted_scores.A\n","\n","    if config['weighting'] == 'col_wise':\n","        # R_rw = Dk^-1*K*A\n","        # R_rw = R_unw*weights\n","        weights = np.abs(user_similarity).sum(axis=0).A.squeeze()\n","        Dk1 = np.divide(1., weights, where=weights!=0)\n","        scores = user_similarity @ diags(Dk1) @ user_item_mtx\n","        return  scores.A\n","\n","\n","    raise ValueError('Unrecognized weighting scheme')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecgfdPTFDSjS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701713321927,"user_tz":-180,"elapsed":25523,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"ccffdc84-2f27-4c8d-897f-819ed2c0f6ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["##########_____unweihgted_____##########\n","Time: 6.02\n","##########_____colwise_weihgted_____##########\n","Time: 11.74\n","##########_____unweihgted_trunc20_____##########\n","Time: 3.72\n","##########_____colwise_weihgted_trunc20_____##########\n","Time: 3.66\n","CPU times: user 20.9 s, sys: 775 ms, total: 21.7 s\n","Wall time: 25.1 s\n"]}],"source":["%%time\n","import timeit\n","\n","n_neighbors = 20\n","models_names_list = ['unweihgted', 'colwise_weihgted']\n","models_names_list += [i+f'_trunc{n_neighbors}' for i in models_names_list]\n","\n","conf_list = [{'weighting': None, 'n_neighbors': None},\n","             {'weighting': 'col_wise', 'n_neighbors': None},\n","             {'weighting': None, 'n_neighbors': n_neighbors},\n","             {'weighting': 'col_wise', 'n_neighbors': n_neighbors}\n","             ]\n","models_param_list = []\n","models_scores_list = []\n","\n","for name, conf in zip(models_names_list, conf_list):\n","    print(f'##########_____{name}_____##########')\n","    start = timeit.default_timer()\n","\n","    uknn_params = build_uknn_model(conf, trainset, data_description)\n","    uknn_scores = uknn_model_scoring(uknn_params, testset, data_description)\n","    models_param_list.append(uknn_params)\n","    models_scores_list.append(uknn_scores)\n","\n","    stop = timeit.default_timer()\n","    print(f'Time: {stop - start:.2f}')"]},{"cell_type":"markdown","metadata":{"id":"lRQtbJ-KDSjU"},"source":["### Evaluation (1 pts)"]},{"cell_type":"markdown","metadata":{"id":"_gedTK4XDSjU"},"source":["Generate recommendations for both models and Calculate metrics"]},{"cell_type":"code","source":["metrics_list = []\n","\n","for name, conf, scores in zip(models_names_list, conf_list, models_scores_list):\n","    print(f'##########_____{name}_____##########')\n","    print(conf)\n","\n","    uknn_scores = scores.copy()\n","    # delete scored items\n","    downvote_seen_items(uknn_scores, testset, data_description)\n","    # take topN recomendastions\n","    uknn_recs = topn_recommendations(uknn_scores)\n","\n","    metrics = model_evaluate(uknn_recs[holdout_reind.userid.tolist()], holdout_reind, data_description, topn=10)\n","    metrics_list.append(metrics)\n","    print('HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"myDDPJOMq0RU","executionInfo":{"status":"ok","timestamp":1701713321928,"user_tz":-180,"elapsed":14,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"327dad6a-18dc-47fb-8faa-431cbac2ecea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##########_____unweihgted_____##########\n","{'weighting': None, 'n_neighbors': None}\n","HR=0.0443, MRR=0.0175, COV=0.0515\n","\n","##########_____colwise_weihgted_____##########\n","{'weighting': 'col_wise', 'n_neighbors': None}\n","HR=0.0492, MRR=0.0186, COV=0.0502\n","\n","##########_____unweihgted_trunc20_____##########\n","{'weighting': None, 'n_neighbors': 20}\n","HR=0.0541, MRR=0.0167, COV=0.166\n","\n","##########_____colwise_weihgted_trunc20_____##########\n","{'weighting': 'col_wise', 'n_neighbors': 20}\n","HR=0.0603, MRR=0.016, COV=0.237\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"1TwvAhJCDSjV"},"source":["### Tuning (2 pts)\n","- Try to find a neighborhood size that gives you better results.\n","- Perform a simple grid-search experiment and report your findings.\n","- Optional: try improving results with a different similarity measure."]},{"cell_type":"markdown","source":["We will gridsearch our neighbors using the best model"],"metadata":{"id":"IH07iKQ4CaGs"}},{"cell_type":"code","source":["n_neighbors_list =  list(range(1, 60))\n","\n","for n_neighbors in n_neighbors_list:\n","    print(f'##########_____N_neighbors:{n_neighbors}_____##########')\n","    start = timeit.default_timer()\n","\n","    conf = {'weighting': 'col_wise', 'n_neighbors': n_neighbors}\n","    uknn_params = build_uknn_model(conf, trainset, data_description)\n","    uknn_scores = uknn_model_scoring(uknn_params, testset, data_description)\n","\n","    # delete scored items\n","    downvote_seen_items(uknn_scores, testset, data_description)\n","    # take topN recomendastions\n","    uknn_recs = topn_recommendations(uknn_scores)\n","\n","    metrics = model_evaluate(uknn_recs[holdout_reind.userid.tolist()], holdout_reind, data_description, topn=10)\n","    print('HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics))\n","\n","    stop = timeit.default_timer()\n","    print(f'Time: {stop - start:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ESDIZx2iCOKR","executionInfo":{"status":"ok","timestamp":1701714298714,"user_tz":-180,"elapsed":252500,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"63174c44-ddff-48f9-ec87-74cb76f14d52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##########_____N_neighbors:1_____##########\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n","  self._set_arrayXarray(i, j, x)\n"]},{"output_type":"stream","name":"stdout","text":["HR=0.0234, MRR=0.00838, COV=0.407\n","\n","Time: 3.58\n","##########_____N_neighbors:2_____##########\n","HR=0.048, MRR=0.0188, COV=0.34\n","\n","Time: 3.22\n","##########_____N_neighbors:3_____##########\n","HR=0.0443, MRR=0.0138, COV=0.32\n","\n","Time: 4.26\n","##########_____N_neighbors:4_____##########\n","HR=0.0529, MRR=0.0158, COV=0.311\n","\n","Time: 3.42\n","##########_____N_neighbors:5_____##########\n","HR=0.0504, MRR=0.0146, COV=0.298\n","\n","Time: 3.25\n","##########_____N_neighbors:6_____##########\n","HR=0.0517, MRR=0.0143, COV=0.289\n","\n","Time: 3.33\n","##########_____N_neighbors:7_____##########\n","HR=0.0443, MRR=0.0156, COV=0.279\n","\n","Time: 4.87\n","##########_____N_neighbors:8_____##########\n","HR=0.048, MRR=0.0172, COV=0.275\n","\n","Time: 3.96\n","##########_____N_neighbors:9_____##########\n","HR=0.0517, MRR=0.0201, COV=0.274\n","\n","Time: 4.32\n","##########_____N_neighbors:10_____##########\n","HR=0.0529, MRR=0.0195, COV=0.267\n","\n","Time: 6.59\n","##########_____N_neighbors:11_____##########\n","HR=0.0517, MRR=0.0186, COV=0.264\n","\n","Time: 4.07\n","##########_____N_neighbors:12_____##########\n","HR=0.0541, MRR=0.0173, COV=0.263\n","\n","Time: 4.62\n","##########_____N_neighbors:13_____##########\n","HR=0.0504, MRR=0.0163, COV=0.26\n","\n","Time: 3.57\n","##########_____N_neighbors:14_____##########\n","HR=0.048, MRR=0.0161, COV=0.257\n","\n","Time: 4.90\n","##########_____N_neighbors:15_____##########\n","HR=0.0455, MRR=0.0158, COV=0.256\n","\n","Time: 4.06\n","##########_____N_neighbors:16_____##########\n","HR=0.0517, MRR=0.0169, COV=0.252\n","\n","Time: 4.21\n","##########_____N_neighbors:17_____##########\n","HR=0.0504, MRR=0.0151, COV=0.248\n","\n","Time: 5.83\n","##########_____N_neighbors:18_____##########\n","HR=0.0541, MRR=0.0156, COV=0.243\n","\n","Time: 3.93\n","##########_____N_neighbors:19_____##########\n","HR=0.0603, MRR=0.0171, COV=0.241\n","\n","Time: 3.43\n","##########_____N_neighbors:20_____##########\n","HR=0.0603, MRR=0.016, COV=0.237\n","\n","Time: 3.98\n","##########_____N_neighbors:21_____##########\n","HR=0.0504, MRR=0.0159, COV=0.236\n","\n","Time: 4.69\n","##########_____N_neighbors:22_____##########\n","HR=0.0492, MRR=0.0157, COV=0.236\n","\n","Time: 4.06\n","##########_____N_neighbors:23_____##########\n","HR=0.0541, MRR=0.0171, COV=0.237\n","\n","Time: 4.13\n","##########_____N_neighbors:24_____##########\n","HR=0.0529, MRR=0.0175, COV=0.235\n","\n","Time: 6.99\n","##########_____N_neighbors:25_____##########\n","HR=0.0554, MRR=0.0176, COV=0.232\n","\n","Time: 5.61\n","##########_____N_neighbors:26_____##########\n","HR=0.0529, MRR=0.0182, COV=0.234\n","\n","Time: 4.28\n","##########_____N_neighbors:27_____##########\n","HR=0.0455, MRR=0.016, COV=0.233\n","\n","Time: 5.80\n","##########_____N_neighbors:28_____##########\n","HR=0.0455, MRR=0.0172, COV=0.227\n","\n","Time: 4.83\n","##########_____N_neighbors:29_____##########\n","HR=0.0492, MRR=0.0175, COV=0.227\n","\n","Time: 5.12\n","##########_____N_neighbors:30_____##########\n","HR=0.0504, MRR=0.0174, COV=0.23\n","\n","Time: 4.69\n","##########_____N_neighbors:31_____##########\n","HR=0.0504, MRR=0.0183, COV=0.229\n","\n","Time: 3.96\n","##########_____N_neighbors:32_____##########\n","HR=0.0492, MRR=0.0183, COV=0.23\n","\n","Time: 4.66\n","##########_____N_neighbors:33_____##########\n","HR=0.0541, MRR=0.0191, COV=0.229\n","\n","Time: 4.70\n","##########_____N_neighbors:34_____##########\n","HR=0.0492, MRR=0.0176, COV=0.226\n","\n","Time: 4.60\n","##########_____N_neighbors:35_____##########\n","HR=0.048, MRR=0.0185, COV=0.222\n","\n","Time: 4.18\n","##########_____N_neighbors:36_____##########\n","HR=0.0492, MRR=0.0198, COV=0.225\n","\n","Time: 3.88\n","##########_____N_neighbors:37_____##########\n","HR=0.048, MRR=0.0188, COV=0.22\n","\n","Time: 4.57\n","##########_____N_neighbors:38_____##########\n","HR=0.0492, MRR=0.02, COV=0.219\n","\n","Time: 3.57\n","##########_____N_neighbors:39_____##########\n","HR=0.0517, MRR=0.0191, COV=0.218\n","\n","Time: 3.46\n","##########_____N_neighbors:40_____##########\n","HR=0.0504, MRR=0.0187, COV=0.213\n","\n","Time: 4.48\n","##########_____N_neighbors:41_____##########\n","HR=0.0492, MRR=0.0192, COV=0.215\n","\n","Time: 4.35\n","##########_____N_neighbors:42_____##########\n","HR=0.048, MRR=0.0192, COV=0.215\n","\n","Time: 3.56\n","##########_____N_neighbors:43_____##########\n","HR=0.0467, MRR=0.0191, COV=0.212\n","\n","Time: 3.44\n","##########_____N_neighbors:44_____##########\n","HR=0.0455, MRR=0.0184, COV=0.213\n","\n","Time: 4.23\n","##########_____N_neighbors:45_____##########\n","HR=0.0455, MRR=0.018, COV=0.212\n","\n","Time: 3.82\n","##########_____N_neighbors:46_____##########\n","HR=0.0529, MRR=0.0196, COV=0.211\n","\n","Time: 3.40\n","##########_____N_neighbors:47_____##########\n","HR=0.0554, MRR=0.0212, COV=0.212\n","\n","Time: 3.40\n","##########_____N_neighbors:48_____##########\n","HR=0.0517, MRR=0.0179, COV=0.213\n","\n","Time: 4.56\n","##########_____N_neighbors:49_____##########\n","HR=0.0529, MRR=0.0188, COV=0.213\n","\n","Time: 3.92\n","##########_____N_neighbors:50_____##########\n","HR=0.0541, MRR=0.0185, COV=0.21\n","\n","Time: 3.51\n","##########_____N_neighbors:51_____##########\n","HR=0.059, MRR=0.0199, COV=0.207\n","\n","Time: 3.53\n","##########_____N_neighbors:52_____##########\n","HR=0.059, MRR=0.0195, COV=0.206\n","\n","Time: 4.79\n","##########_____N_neighbors:53_____##########\n","HR=0.0627, MRR=0.0196, COV=0.203\n","\n","Time: 4.10\n","##########_____N_neighbors:54_____##########\n","HR=0.0578, MRR=0.0196, COV=0.202\n","\n","Time: 4.72\n","##########_____N_neighbors:55_____##########\n","HR=0.0603, MRR=0.0202, COV=0.203\n","\n","Time: 4.68\n","##########_____N_neighbors:56_____##########\n","HR=0.0627, MRR=0.0213, COV=0.199\n","\n","Time: 4.15\n","##########_____N_neighbors:57_____##########\n","HR=0.0578, MRR=0.0213, COV=0.198\n","\n","Time: 3.56\n","##########_____N_neighbors:58_____##########\n","HR=0.0566, MRR=0.0214, COV=0.199\n","\n","Time: 3.63\n","##########_____N_neighbors:59_____##########\n","HR=0.0578, MRR=0.0211, COV=0.2\n","\n","Time: 5.07\n"]}]},{"cell_type":"markdown","source":["We get pretty good results with n= 4,5,6. The best is in n=19,20, and n=51-56\n","\n","In the first option, thanks to this, we cut off unnecessary neighbors. In the second, apparently with such an average hold, we capture the information we need from their previously discarded users, significantly improving the metric. And third take a lot of users and take stability plato arund 0.057-0.06"],"metadata":{"id":"6j9EoTCSEDGY"}},{"cell_type":"markdown","metadata":{"id":"ZAazoitaDSjW"},"source":["## Final analysis (3 pts)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"3hNHGhbqFqJ-"}},{"cell_type":"markdown","metadata":{"id":"-eGvS59xDSjW"},"source":["1. Provide an analysis on which model performs the best and explain why.\n","2. Explain the difference in computational complexity of your models. Consider how the training and the recommendation generation differ for different models in terms of\n","    - the amount of RAM,\n","    - the amount of disk storage,\n","    - the load on CPU.\n","3. How else would you modify the model to improve either the quality of recommendations or computational performance? Describe at least one modification and its envisioned effect."]},{"cell_type":"markdown","source":["1\n","\n","The best performance was shown by models with weak generalisation.Which is logical since with such generalization we train our model on data about the user on whom we will test the metrics in the future, compared to strong generalization when we use warm start. Of cause it isnt cold start, but model didnot fitted on test data"],"metadata":{"id":"n5fXfkq7IYaX"}},{"cell_type":"markdown","source":["2\n","\n","We have fairly similar models that are not that different.\n","\n","During the training process, we create an intersection matrix from a dataframe and calculate the $K$ similarity matrix $O(m^2*n)$. This happens in both strong and weak generalization, the only thing in strong generalization is that the value $m$ is a little less since we removed test users from there.\n","\n","When receiving recommendations (weak generalisation), the complexity in the unweigted version is simpler since we simply multiply two matrices $K*A$, but in wheghted case we take product of 3 matrix $K*D^-1*A$, where $D^-1$ is also must be compute. So it was more hard variant for time/memory/cpu.\n","\n","The strong  generalisation recommendation recive is lighter cause we do computation only with test simularity matrix $Ktest$, which is significantly less $K$ (less time, cpu, memory)(compare time duration)\n","\n","P.S: But it is worth considering that we use neighbor sampling methods, which simplifies calculations when building and predicting models (compare time duration)\n","\n"],"metadata":{"id":"Cj3q_nJeLF6S"}},{"cell_type":"markdown","source":["3\n","\n","Of course, the first thing that comes to mind is to add contextual information to the model. So that when building a model, information not only on views (ratings), but also other additional information about users and films (genre, director, city, age, etc.) is taken into account. In this way it is possible to improve the values of the calculated.\n","\n","  Also we can use different simmularity function, not only cosine."],"metadata":{"id":"wSCsubx9LG5c"}},{"cell_type":"markdown","source":["# Problem 3 (20 pts)"],"metadata":{"id":"lU_Jjn2UYbeo"}},{"cell_type":"markdown","source":["* Using the code from seminars, implement efficient version of a weighted matrix factorization based on SGD for the strong generalization test. Reuse the code from the KNN part of the homework. Recall, you need to implement folding-in for this scenario.\n","\n","* To make the model learn better, implement the negative sampling as well. You can use 1:1 ratio of negative samples, i.e., 1 negative example for each positive example.\n","* Try slightly tuning the hyper-parameters of the model (i.e., rank, learning rate, regularization, negative samples ratio) to obtain better recommendations quality.\n","* Report the results."],"metadata":{"id":"xPtQWQniyphg"}},{"cell_type":"code","source":["def generate_interactions_matrix(data, data_description, rebase_users=False):\n","    \"csr matrix user-item intersection, value of cell is rating by user\"\n","    '''\n","    Convert pandas dataframe with interactions into a sparse matrix.\n","    Allows reindexing user ids, which help ensure data consistency\n","    at the scoring stage (assumes user ids are sorted in scoring array).\n","    '''\n","    n_users = data_description['n_users']\n","    n_items = data_description['n_items']\n","    # get indices of observed data\n","    user_idx = data[data_description['users_col']].values # type your code here\n","    user_index = []\n","    if rebase_users:\n","        user_idx, user_index = pd.factorize(user_idx, sort=True)\n","        n_users = len(user_index)\n","    item_idx = data[data_description['items_col']].values # type your code here\n","    feedback = data[data_description['feedback_col']].values # type your code here\n","    # construct rating matrix\n","    return csr_matrix((feedback, (user_idx, item_idx)), shape=(n_users, n_items)), user_index"],"metadata":{"id":"4-G0F7BCRxvg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download data, and reindex users and items"],"metadata":{"id":"wqGECimrzEXE"}},{"cell_type":"code","source":["data_ = get_movielens_data(include_time=True)\n","data, data_index = transform_indices(data_, 'userid', 'movieid')"],"metadata":{"id":"qlXeq67OYctE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_description = dict(\n","    users_col = 'userid',\n","    items_col = 'movieid',\n","    feedback_col = 'rating',\n","    n_users = len(data.userid.unique()),\n","    n_items = len(data.movieid.unique())\n",")\n","data_description"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnlMDj-12xGL","executionInfo":{"status":"ok","timestamp":1701112901830,"user_tz":-180,"elapsed":7,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"f603e8b9-bd3e-4cf7-b032-4108de884b33"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'users_col': 'userid',\n"," 'items_col': 'movieid',\n"," 'feedback_col': 'rating',\n"," 'n_users': 6040,\n"," 'n_items': 3706}"]},"metadata":{},"execution_count":263}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYkUPGuYdiX7"},"outputs":[],"source":["def split_by_time(data, time_q=0.95, timeid='timestamp'):\n","    '''\n","    Split the input `data` DataFrame into two parts based on the timestamp, with the split point\n","    being determined by the quantile value `time_q`. The function returns a tuple `(before, after)`\n","    containing the two DataFrames. The `after` DataFrame contains the rows with timestamps greater\n","    than or equal to the split point, while the `before` DataFrame contains the remaining rows.\n","\n","    Details:\n","    The `quantile` method of the pandas DataFrame is used to calculate the time point (i.e., timestamp)\n","    that divides the data into two parts based on the given quantile value `time_q`. Specifically,\n","    the time point `split_timepoint` is calculated as the `time_q`th quantile of the values in the `timeid`\n","    column of the `data` DataFrame, using the interpolation method of `nearest`. This means that\n","    `split_timepoint` is the timestamp at or immediately after which `time_q` percent of the data points occur.\n","    '''\n","    split_timepoint = data[timeid].quantile(q=time_q, interpolation='nearest')\n","    after = data.query(f'{timeid} >= @split_timepoint')\n","    before = data.drop(after.index)\n","    return before, after"]},{"cell_type":"markdown","source":["Split data base on split_time to before and after"],"metadata":{"id":"QAOke2XzzYaA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OYDuo9rdiX8"},"outputs":[],"source":["before, after = split_by_time(data, time_q=0.95)"]},{"cell_type":"markdown","source":["Make holdout sampling on after to create holdout data and testset_part_1"],"metadata":{"id":"7FVPFJgQzocm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw7IbKBqdiX8"},"outputs":[],"source":["testset_part_1, holdout_ = leave_last_out(after, userid='userid', timeid='timestamp')\n","\n","# verify correctness of time-based splitting,\n","# i.e., for each test user, the holdout contains only future interactions w.r.t to testset\n","verify_time_split(testset_part_1, holdout_)\n","assert len(set(testset_part_1.userid.unique()) - set(holdout_.userid.unique()))==0\n","\n","testset_part_2 = before[before.userid.isin(after.userid.unique())]"]},{"cell_type":"markdown","source":["Then for strong generalisation we must delete all after(holdout) users from before data history, so we get our trainset"],"metadata":{"id":"YsE8aRPN0O-5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"np82hUDAdiX9"},"outputs":[],"source":["trainset_ = before[~before.userid.isin(after.userid.unique())]"]},{"cell_type":"markdown","source":["And all deleted rows is our testset_part_2"],"metadata":{"id":"pScSBDf50WAy"}},{"cell_type":"code","source":["testset_part_2 = before[before.userid.isin(after.userid.unique())]"],"metadata":{"id":"EF7NPzpl0Y92"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Combine testset_part_1 and testset_part_2 we create our testset wich use to make prediction for test users"],"metadata":{"id":"xU3INUNz0gFA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PUdTNMmdiX9"},"outputs":[],"source":["# combine all test users data into a single `testset_` Dataframe.\n","testset_ = pd.concat([testset_part_1, testset_part_2], axis=0, ignore_index=False)\n","assert len(set(testset_.userid)) == len((set(holdout_.userid)))\n","assert len(set(testset_.userid).difference((set(holdout_.userid)))) == 0\n","assert len(set(holdout_.userid).difference((set(testset_.userid)))) == 0"]},{"cell_type":"markdown","metadata":{"id":"YD-3sMqhdiX-"},"source":["Building internal representation of user and item index. We must building a contiguous index for train users starting from 0 cause for matrix representation fit form. Dont chane movieid, it was reindexed when download data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8xNyAMe2diX-"},"outputs":[],"source":["# reindex data to make contiguous index starting from 0 for user and item IDs\n","#trainset, train_data_index = transform_indices(trainset_, 'userid', 'movieid') нам не нужно реиндексировать фильмы\n","\n","train_user_idx, train_user_index = pd.factorize(trainset_['userid'], sort=True)\n","\n","trainset = trainset_.copy()\n","trainset['userid'] = train_user_idx\n","assert len(set(trainset.userid)) == len(set(trainset_.userid))"]},{"cell_type":"markdown","source":["Write function wich can create negative samplings"],"metadata":{"id":"xOM7QhWB1MS3"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","def create_negative_samples(data, data_description, rating=0, gamma=1):\n","    itemsid_set = set(range(data_description['n_items']))\n","\n","    df_users_no_rating_items_ = data.groupby(['userid']).agg({'movieid': lambda x: np.random.choice(\n","        list(itemsid_set-set(x)),\n","        size=int(gamma*len(set(x))),\n","        replace=False).tolist()})\n","\n","    df_users_no_rating_items = df_users_no_rating_items_.reset_index()\n","    negative_samples = []\n","    #assert df_users_no_rating_items.index.tolist() == data.userid.unique().tolist()\n","\n","    for user_id, noitems_list in df_users_no_rating_items.values:\n","      for item_id in noitems_list:\n","        negative_samples.append([user_id, item_id, rating])\n","\n","    data_negative = pd.DataFrame(negative_samples, columns=['userid', 'movieid', 'rating'])\n","    return data_negative"],"metadata":{"id":"Fub3nvEIuHvm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create trainset_with_negative"],"metadata":{"id":"EieAkOlm1Th9"}},{"cell_type":"code","source":["trainset_negative = create_negative_samples(trainset, data_description, rating=0, gamma=1)\n","trainset_with_negative = pd.concat([trainset,trainset_negative])"],"metadata":{"id":"gajGKEPul3QW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_description = dict(\n","    users_col = 'userid',\n","    items_col = 'movieid',\n","    feedback_col = 'rating',\n","    n_users = len(train_user_index),\n","    n_items = data_description['n_items']\n",")\n","\n","sgd_config = dict(\n","    learning_rate = 0.002,\n","    regularization = 1,\n","    n_epochs = 25,\n","    rank = 35,\n","    seed = 16\n",")"],"metadata":{"id":"bD4LPmKNmaCf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our main implementation for  Weighted matrix\n","factorization based on SGD (weak weighted)"],"metadata":{"id":"YZmwMurk1m7P"}},{"cell_type":"code","source":["from numba import njit, objmode, prange\n","\n","\n","def mf_sgd_build(config, data, data_description):\n","    useridx = data[data_description['users_col']].values\n","    itemidx = data[data_description['items_col']].values\n","    ratings = data[data_description['feedback_col']].values\n","    learning_rate = config['learning_rate']\n","    regularization = config['regularization']\n","    n_epochs = config['n_epochs']\n","    rank = config['rank']\n","\n","    n_users = data_description['n_users']\n","    n_items = data_description['n_items']\n","    rng = np.random.default_rng(config.get('seed', None))\n","\n","    P, Q, mse_history, bestP, bestQ  = sgd_epochs(\n","        useridx, itemidx, ratings,\n","        learning_rate, regularization, n_epochs,\n","        rank, n_users, n_items,\n","        rng\n","    )\n","    return P, Q, mse_history, bestP, bestQ\n","\n","@njit\n","def sgd_epochs(\n","    useridx, itemidx, ratings,\n","    learning_rate, regularization, n_epochs,\n","    rank, n_users, n_items,\n","    rng\n","):\n","    P = rng.normal(0, 0.01, (n_users, rank))\n","    Q = rng.normal(0, 0.01, (n_items, rank))\n","    print('SGD start')\n","    best_mse = 1e3\n","    history = []\n","    for epoch in range(n_epochs):\n","        mse = sgd_step(P, Q, useridx, itemidx, ratings, learning_rate, regularization, rng)\n","        history.append(mse)\n","        if mse < best_mse:\n","            best_mse = mse\n","            bestP, bestQ = P, Q\n","    return P, Q, history, bestP, bestQ\n","\n","def check_metric_growth(testset, holdout, data_description):\n","    def update_target_metric(metrics):\n","        hr, mrr, cov = metrics\n","        eval_callback.target_metrics.append(hr)\n","\n","    def eval_callback(epoch, P, Q):\n","        mf_params = P, Q, None\n","        sgd_scores = mf_sgd_scoring(mf_params, None, data_description)\n","        downvote_seen_items(sgd_scores, testset, data_description)\n","        sgd_recs = topn_recommendations(sgd_scores, topn=10)\n","        metrics = model_evaluate(sgd_recs, holdout, data_description)\n","        update_target_metric(metrics)\n","        stopping_criteria = 1\n","        if len(eval_callback.target_metrics) >= 2:\n","            stopping_criteria = eval_callback.target_metrics[-1] > eval_callback.target_metrics[-2]\n","        return int(stopping_criteria)\n","    eval_callback.target_metrics = []\n","    return eval_callback\n","\n","@njit\n","def sgd_step(P, Q, useridx, itemidx, ratings, learning_rate, regularization, rng):\n","    n_interactions = len(ratings)\n","    squared_err = 0.\n","\n","    # cause we idx by exist intersections, so we use weak weighting scheeme\n","    for idx in rng.permutation(n_interactions):\n","        userid = useridx[idx]\n","        itemid = itemidx[idx]\n","        rating = ratings[idx]\n","\n","        pi = P[userid]\n","        qj = Q[itemid]\n","        error = rating - pi @ qj\n","\n","        pi += learning_rate * (error*qj - regularization*pi)\n","        qj += learning_rate * (error*pi - regularization*qj)\n","\n","        squared_err += error*error\n","\n","    mse = squared_err / n_interactions\n","    return mse\n","\n","def mf_sgd_scoring(params, data, data_description):\n","    P, Q, _ = params\n","    test_users = data_description['test_users']\n","    scores = P[test_users] @ Q.T\n","    return scores"],"metadata":{"id":"Zlym3TtbmOIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train our model to create P and Q matricies"],"metadata":{"id":"l6EHJcKY1kTx"}},{"cell_type":"code","source":["%%time\n","sgd_params = mf_sgd_build(sgd_config, trainset, train_data_description)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MGr8z4JdoiUv","executionInfo":{"status":"ok","timestamp":1701106953600,"user_tz":-180,"elapsed":16929,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"1f3fee6f-af8d-48a9-e118-764f3526bb26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 14.3 s, sys: 84.6 ms, total: 14.4 s\n","Wall time: 16.9 s\n"]}]},{"cell_type":"code","source":["_, _, _, P, Q = sgd_params\n","print('MSE_history:', sgd_params[2])\n","assert Q.shape[0] == data_description['n_items']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"piyeYU_XqlDm","executionInfo":{"status":"ok","timestamp":1701106953600,"user_tz":-180,"elapsed":6,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"13a41ca4-8fe9-4e06-99b7-40078167ddce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE_history: [14.151049458218015, 14.14813069205689, 14.036691091129663, 11.657425864984777, 5.674298276504863, 3.368613271996213, 2.649693742459554, 2.335175087785384, 2.1699710284773532, 2.07051803255666, 2.007889219652979, 1.9674925556202039, 1.9387958003683479, 1.9168311362855928, 1.901736670786542, 1.8897402345114196, 1.882402770906767, 1.8742419307232039, 1.8684641680793952, 1.8664384916481511]\n"]}]},{"cell_type":"markdown","source":["We re-index test users so that we can then create an intersection matrix. But don’t forget to also correct holdout_ later when we predict the results"],"metadata":{"id":"a0KB_55D8t3L"}},{"cell_type":"code","source":["test_data_description = dict(\n","    users_col = 'userid',\n","    items_col = 'movieid',\n","    feedback_col = 'rating',\n","    n_users = len(testset_.userid.unique()),\n","    n_items = data_description['n_items']\n",")\n","print(test_data_description)\n","\n","test_int_matrix, test_user_index = generate_interactions_matrix(testset_, test_data_description, rebase_users=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-l8cjyszc5D","executionInfo":{"status":"ok","timestamp":1701107012632,"user_tz":-180,"elapsed":307,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"a68f982a-dfb6-4fba-968d-66214bae8de5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'users_col': 'userid', 'items_col': 'movieid', 'feedback_col': 'rating', 'n_users': 813, 'n_items': 3706}\n"]}]},{"cell_type":"markdown","source":["Write function wich get p vector and make prediction for new user"],"metadata":{"id":"qV1dGAYI4XM4"}},{"cell_type":"code","source":["def create_newuser_p_vector(new_user_intr_vector, Q, config):\n","    regularization = config['regularization']\n","    learning_rate = config['learning_rate']\n","    n_epohs = config['n_epohs']\n","\n","    assert len(new_user_intr_vector.shape) == 1\n","    item_indexes = np.nonzero(new_user_intr_vector)[0]\n","\n","    rng = np.random.default_rng(25)\n","    pi  = rng.normal(0, 0.01, (Q.shape[1])) # shape (d,)\n","    rating = new_user_intr_vector.copy()\n","\n","    for i in range(n_epohs):\n","      np.random.shuffle(item_indexes)\n","      for idx in item_indexes:\n","          qj = Q[idx] # shape (d,)\n","          error = rating[idx] - pi @ qj # number\n","          pi += learning_rate * (error*qj - regularization*pi)\n","\n","    ratingsi = pi @ Q.T\n","    return pi, ratingsi"],"metadata":{"id":"sSHHFPVJqmG7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make prediction for all test users"],"metadata":{"id":"QNLiyoeM40Ou"}},{"cell_type":"code","source":["%%time\n","compute_new_user_p_config = dict(\n","    regularization = 1,\n","    learning_rate = 0.003,\n","    n_epohs = 25\n",")\n","\n","test_p_vectors = np.zeros((test_int_matrix.A.shape[0], sgd_config['rank']))\n","test_rating_pred = np.zeros((test_int_matrix.A.shape[0], data_description['n_items']))\n","for i in tqdm.tqdm(range(test_int_matrix.A.shape[0])):\n","    p_i, ratings_i = create_newuser_p_vector(test_int_matrix.A[i], Q=Q, config=compute_new_user_p_config)\n","    test_p_vectors[i] = p_i\n","    test_rating_pred[i] = ratings_i"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xyj80XE791G5","executionInfo":{"status":"ok","timestamp":1701107265378,"user_tz":-180,"elapsed":123501,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"8294e657-7f50-4f3f-edd1-92e1029f2d49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 813/813 [02:03<00:00,  6.61it/s]"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 42s, sys: 46.1 s, total: 2min 28s\n","Wall time: 2min 3s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["Delete items from recomendation wich user was scoring"],"metadata":{"id":"SHcjnWQS43zG"}},{"cell_type":"code","source":["def downvote_seen_items(scores, data, data_description, test_user_index=test_user_index):\n","    assert isinstance(scores, np.ndarray), 'Scores must be a dense numpy array!'\n","    itemid = data_description['items_col']\n","    userid = data_description['users_col']\n","    # get indices of observed data, corresponding to scores array\n","    # we need to provide correct mapping of rows in scores array into\n","    # the corresponding user index (which is assumed to be sorted)\n","    row_idx, test_users = pd.factorize(data[userid], sort=True)\n","    assert len(test_users) == scores.shape[0] == data_description['n_users']\n","    assert list(test_user_index) == list(test_users)\n","    col_idx = data[itemid].values\n","    # downvote scores at the corresponding positions\n","    scores[row_idx, col_idx] = scores.min() - 1"],"metadata":{"id":"MTV-y6s0EyDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sgd_scores = test_rating_pred.copy()\n","downvote_seen_items(sgd_scores, testset_, test_data_description)"],"metadata":{"id":"jvbroU8MCQe6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Take topN recomendations"],"metadata":{"id":"oYV2Lp2G5Hl6"}},{"cell_type":"code","source":["sgd_recs = topn_recommendations(sgd_scores, topn=10)"],"metadata":{"id":"uBXvXB5OEWfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before valuate our model we need reindex by test_user_index our holdout data and sort it to male consistency"],"metadata":{"id":"SdNNAYxxMSi9"}},{"cell_type":"code","source":["holdout_reind = holdout_.copy()\n","hold_user_indx, hold_user_index = pd.factorize(holdout_.userid, sort=True)\n","assert list(hold_user_index) == list(test_user_index)\n","holdout_reind['userid'] = hold_user_indx"],"metadata":{"id":"Z8fK8rE1MtS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function calculate metrics"],"metadata":{"id":"Su0r7yX35TZZ"}},{"cell_type":"code","source":["def model_evaluate(recommended_items, holdout, holdout_description, topn=10):\n","    itemid = holdout_description['items_col']\n","    holdout_items = holdout[itemid].values\n","    assert recommended_items.shape[0] == len(holdout_items)\n","    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n","    # HR calculation\n","    hr = np.mean(hits_mask.any(axis=1))\n","    # MRR calculation\n","    n_test_users = recommended_items.shape[0]\n","    hit_rank = np.where(hits_mask)[1] + 1.0\n","    mrr = np.sum(1 / hit_rank) / n_test_users\n","    # coverage calculation\n","    n_items = holdout_description['n_items']\n","    cov = np.unique(recommended_items).size / n_items\n","    return hr, mrr, cov"],"metadata":{"id":"nMAFgLNrHZ_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics = model_evaluate(sgd_recs, holdout_reind.sort_values(by=['userid']), test_data_description, topn=10)\n","print(f'HR: {metrics[0]}, \\nMRR: {metrics[1]}, \\nCOV: {metrics[2]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8stpERH1H0aP","executionInfo":{"status":"ok","timestamp":1701107521627,"user_tz":-180,"elapsed":258,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"6cadbc44-276a-424e-83ad-03ab1d18d48a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["HR: 0.01968019680196802, \n","MRR: 0.004327983756028037, \n","COV: 0.01645979492714517\n"]}]},{"cell_type":"markdown","source":["### Test several variants of models"],"metadata":{"id":"gqDv8G3R5v6S"}},{"cell_type":"code","source":["trainset_with_negative05 = pd.concat([trainset, create_negative_samples(trainset, data_description, rating=0, gamma=0.5)])"],"metadata":{"id":"IvssJHGM_9D1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_names = [\"sgd_n20_r25_pn15\",\n","                \"sgd_n20_r25_pn15_negative1\",\n","                \"sgd_n20_r25_pn15_negative0.5\",\n","                \"sgd_n40_r25_pn15\",\n","                \"sgd_n20_r50_pn15\",\n","                \"sgd_n20_r25_pn30\",\n","                \"sgd_n40_r50_pn15_negative1\",\n","                \"sgd_n40_r50_pn15_negative0.5\"\n","                ]\n","\n","sgd_config_list = [{'learning_rate': 0.002, 'regularization': 1, 'n_epochs': 20, 'rank': 25, 'seed': 16},\n","                   {'learning_rate': 0.002, 'regularization': 1, 'n_epochs': 20, 'rank': 25, 'seed': 16},\n","                   {'learning_rate': 0.002, 'regularization': 1, 'n_epochs': 20, 'rank': 25, 'seed': 16},\n","                   {'learning_rate': 0.002, 'regularization': 1, 'n_epochs': 40, 'rank': 32, 'seed': 16},\n","                   {'learning_rate': 0.002, 'regularization': 1, 'n_epochs': 20, 'rank': 50, 'seed': 16},\n","                   {'learning_rate': 0.002, 'regularization': 1, 'n_epochs': 20, 'rank': 25, 'seed': 16},\n","                   {'learning_rate': 0.002, 'regularization': 1, 'n_epochs': 40, 'rank': 50, 'seed': 16},\n","                   {'learning_rate': 0.002, 'regularization': 1, 'n_epochs': 40, 'rank': 50, 'seed': 16}\n","                    ]\n","\n","compute_new_user_p_config_list = [{'regularization': 1, 'learning_rate': 0.002, 'n_epohs': 15},\n","                                  {'regularization': 1, 'learning_rate': 0.002, 'n_epohs': 15},\n","                                  {'regularization': 1, 'learning_rate': 0.002, 'n_epohs': 15},\n","                                  {'regularization': 1, 'learning_rate': 0.002, 'n_epohs': 15},\n","                                  {'regularization': 1, 'learning_rate': 0.002, 'n_epohs': 15},\n","                                  {'regularization': 1, 'learning_rate': 0.002, 'n_epohs': 30},\n","                                  {'regularization': 1, 'learning_rate': 0.002, 'n_epohs': 15},\n","                                  {'regularization': 1, 'learning_rate': 0.002, 'n_epohs': 15}]\n","trainset_list = [trainset,\n","                 trainset_with_negative,\n","                 trainset_with_negative05,\n","                 trainset,\n","                 trainset,\n","                 trainset,\n","                 trainset_with_negative,\n","                 trainset_with_negative05]"],"metadata":{"id":"-nujyL0_8CxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(list(zip(models_names, sgd_config_list, compute_new_user_p_config_list, trainset_list)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5g88JL25wfM","executionInfo":{"status":"ok","timestamp":1701109112196,"user_tz":-180,"elapsed":5,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"a6a38f68-523b-41fd-8a10-fed0c9562e87"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":243}]},{"cell_type":"code","source":["metrics_list = []\n","for model_name, grid_sgd_config, grid_p_config_list,grid_trainset in list(zip(models_names, sgd_config_list, compute_new_user_p_config_list, trainset_list)):\n","      print(f'#####__{model_name}__#####')\n","      # compute model\n","      _, _, _, P, Q = mf_sgd_build(grid_sgd_config, grid_trainset, train_data_description)\n","\n","      # make predictions on test\n","      test_p_vectors = np.zeros((test_int_matrix.A.shape[0], grid_sgd_config['rank']))\n","      test_rating_pred = np.zeros((test_int_matrix.A.shape[0], data_description['n_items']))\n","      for i in tqdm.tqdm(range(test_int_matrix.A.shape[0])):\n","          p_i, ratings_i = create_newuser_p_vector(test_int_matrix.A[i], Q=Q, config=grid_p_config_list)\n","          test_p_vectors[i] = p_i\n","          test_rating_pred[i] = ratings_i\n","\n","      # clear prediction\n","      sgd_scores = test_rating_pred.copy()\n","      downvote_seen_items(sgd_scores, testset_, test_data_description)\n","      sgd_recs = topn_recommendations(sgd_scores, topn=10)\n","\n","      # calculate metrics\n","      metrics = list(model_evaluate(sgd_recs, holdout_reind.sort_values(by=['userid']), test_data_description, topn=10))\n","      metrics_list.append(metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7WEdn4H6h5S","executionInfo":{"status":"ok","timestamp":1701113199907,"user_tz":-180,"elapsed":208670,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"6d8e86d8-0791-422d-c669-e66fbd21fe70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["#####__sgd_n40_r50_pn15_negative1__#####\n","SGD start\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 813/813 [01:17<00:00, 10.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["#####__sgd_n40_r50_pn15_negative0.5__#####\n","SGD start\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 813/813 [01:15<00:00, 10.81it/s]\n"]}]},{"cell_type":"code","source":["pd.DataFrame(metrics_list, columns=['hr', 'mrr', 'cov'], index=models_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"8qmTwyVvPSAi","executionInfo":{"status":"ok","timestamp":1701113219357,"user_tz":-180,"elapsed":294,"user":{"displayName":"Дима Руденков","userId":"09970520399076229635"}},"outputId":"5e2057ec-2c3c-4de0-9c88-8c4f6a443b8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                   hr       mrr       cov\n","sgd_n20_r25_pn15              0.00861  0.003189  0.016460\n","sgd_n20_r25_pn15_negative1    0.04428  0.016332  0.045872\n","sgd_n20_r25_pn15_negative0.5  0.04182  0.014722  0.040745\n","sgd_n40_r25_pn15              0.01476  0.003507  0.015920\n","sgd_n20_r50_pn15              0.01599  0.003705  0.016460\n","sgd_n20_r25_pn30              0.00861  0.003189  0.016460\n","sgd_n40_r50_pn15_negative1    0.04305  0.014754  0.045062\n","sgd_n40_r50_pn15_negative0.5  0.04305  0.013736  0.041015"],"text/html":["\n","  <div id=\"df-4afeda9e-ec61-4815-b1a7-ed6d3268e911\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hr</th>\n","      <th>mrr</th>\n","      <th>cov</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>sgd_n20_r25_pn15</th>\n","      <td>0.00861</td>\n","      <td>0.003189</td>\n","      <td>0.016460</td>\n","    </tr>\n","    <tr>\n","      <th>sgd_n20_r25_pn15_negative1</th>\n","      <td>0.04428</td>\n","      <td>0.016332</td>\n","      <td>0.045872</td>\n","    </tr>\n","    <tr>\n","      <th>sgd_n20_r25_pn15_negative0.5</th>\n","      <td>0.04182</td>\n","      <td>0.014722</td>\n","      <td>0.040745</td>\n","    </tr>\n","    <tr>\n","      <th>sgd_n40_r25_pn15</th>\n","      <td>0.01476</td>\n","      <td>0.003507</td>\n","      <td>0.015920</td>\n","    </tr>\n","    <tr>\n","      <th>sgd_n20_r50_pn15</th>\n","      <td>0.01599</td>\n","      <td>0.003705</td>\n","      <td>0.016460</td>\n","    </tr>\n","    <tr>\n","      <th>sgd_n20_r25_pn30</th>\n","      <td>0.00861</td>\n","      <td>0.003189</td>\n","      <td>0.016460</td>\n","    </tr>\n","    <tr>\n","      <th>sgd_n40_r50_pn15_negative1</th>\n","      <td>0.04305</td>\n","      <td>0.014754</td>\n","      <td>0.045062</td>\n","    </tr>\n","    <tr>\n","      <th>sgd_n40_r50_pn15_negative0.5</th>\n","      <td>0.04305</td>\n","      <td>0.013736</td>\n","      <td>0.041015</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4afeda9e-ec61-4815-b1a7-ed6d3268e911')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4afeda9e-ec61-4815-b1a7-ed6d3268e911 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4afeda9e-ec61-4815-b1a7-ed6d3268e911');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b0749ee8-7553-42d8-bfb3-ad15ff8c0052\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b0749ee8-7553-42d8-bfb3-ad15ff8c0052')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b0749ee8-7553-42d8-bfb3-ad15ff8c0052 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":275}]},{"cell_type":"markdown","source":["Adding negative sampling significantly improves the model. Increasing training epochs and increasing the rank dimension also helps improve metrics. A significant increase in epochs when obtaining vector p_i of a new user does not have much effect"],"metadata":{"id":"f9bd5VNSE5rf"}},{"cell_type":"markdown","source":["Advantages of WMF over SVD:\n","* Efficiency. SGD is inherently more scalable and computationally efficient than methods that require full matrix factorization like SVD. Suitable for large datasets and online learning scenarios.\n","* Adaptability. Handles sparse and dynamic datasets well. Can be updated incrementally with new data.\n","* Generalization. Negative sampling enhances the model's ability to generalize by learning to differentiate between positive and negative instances."],"metadata":{"id":"X5bxA9cpS7cB"}}],"metadata":{"kernelspec":{"display_name":"rstest","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"64cd544b7330e8e73b8689d110cc075e8c836a404445c2b82c04f3ea96ea86ff"}},"colab":{"provenance":[],"collapsed_sections":["lRQtbJ-KDSjU","1TwvAhJCDSjV"]}},"nbformat":4,"nbformat_minor":0}